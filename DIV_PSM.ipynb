{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP9vZ8FgVIsS"
      },
      "source": [
        "## Classifying text using BERT\n",
        "\n",
        "This week we learn how to classify text into a specific category using Google's Bidirectional Encoder Representations from Transformers (BERT) language model. Besides text classification, BERT is also used in Google's search engine to better understand searches in English. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1jxA4NCV3t4"
      },
      "source": [
        "This week we also install a new package \"ktrain\", which uses keras as a baseline, but allows to create and train models with very few lines of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_WZjPRCCrdj",
        "outputId": "3c87ff7c-3b2a-440c-a2cd-5adc424f53bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ktrain\n",
            "  Downloading ktrain-0.31.2-py3-none-any.whl (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 2.7 MB/s \n",
            "\u001b[?25hCollecting keras-bert>=0.86.0\n",
            "  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n",
            "Collecting syntok==1.3.3\n",
            "  Downloading syntok-1.3.3-py3-none-any.whl (22 kB)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 47.7 MB/s \n",
            "\u001b[?25hCollecting transformers==4.10.3\n",
            "  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 44.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ktrain) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ktrain) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.1.0)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.2.2)\n",
            "Collecting cchardet\n",
            "  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n",
            "\u001b[K     |████████████████████████████████| 263 kB 61.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.3.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 42.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.0.4)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (from ktrain) (0.42.1)\n",
            "Collecting whoosh\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[K     |████████████████████████████████| 468 kB 59.6 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2->ktrain) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2->ktrain) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2->ktrain) (1.21.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from syntok==1.3.3->ktrain) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.3->ktrain) (4.64.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 24.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 15.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 11.0 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 58.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.3->ktrain) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.3->ktrain) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers==4.10.3->ktrain) (4.1.1)\n",
            "Collecting keras-transformer==0.40.0\n",
            "  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
            "Collecting keras-pos-embd==0.13.0\n",
            "  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
            "Collecting keras-multi-head==0.29.0\n",
            "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
            "Collecting keras-layer-normalization==0.16.0\n",
            "  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
            "Collecting keras-position-wise-feed-forward==0.8.0\n",
            "  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
            "Collecting keras-embed-sim==0.10.0\n",
            "  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
            "Collecting keras-self-attention==0.51.0\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (1.4.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->ktrain) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->ktrain) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.10.3->ktrain) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.3->ktrain) (7.1.2)\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, langdetect, sacremoses\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33517 sha256=dafab89aa804784ec0bbc44fd8cbe69aadecbd01766eb5cdbd50dcc05bd9a1cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/e8/45/842b3a39831261aef9154b907eacbc4ac99499a99ae829b06f\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12305 sha256=fb2f3aa82f01d025e07206e7873d3d28b18e82f6c09a83f2e78c311dbaec5c15\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/68/26/692ed21edd832833c3b0a0e21615bcacd99ca458b3f9ed571f\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3960 sha256=db5574406f43dd647faa45fe930df4cb22970379838a35b81232bd01c47d7948\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/67/b5/d847588d075895281e1cf5590f819bd4cf076a554872268bd5\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4668 sha256=22b6d4c0ec9e8a286f073c733ed454cceb69d99dc58665342993bd4f35367363\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/5d/1c/2e619f594f69fbcf8bc20943b27d414871c409be053994813e\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14993 sha256=896f1c6fb3de9aecd4fee8998742bbffb779e4aff5519f16b515f6e02e926c55\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/aa/3c/9d15d24005179dae08ff291ce99c754b296347817d076fd9fb\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6962 sha256=72bd43453a419665d24cdde75935979567a64a675a56fa519129000029ba2f73\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/c1/a0/dc44fcf68c857b7ff6be9a97e675e5adf51022eff1169b042f\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4983 sha256=1a13b03dbfde212dfd401e52d81543cd7c88b49e339af23db7ae1c8259ee9c63\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/75/6f/d42f6e051506f442daeba53ff1e2d21a5f20ef8c411610f2bb\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=a7a0d38e8016bb1ea7cb57d2d0a4ef6a518a3466814e526df63bf55a37d15fa6\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=93c335bbc1146f494babca46200c2a6b52515c49bb32a4c08745704f813bff34\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=ca2e8fdb72a286c1a70d0c4cf16b6533861dd3391ee8c52a49bea383c2ed54b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention langdetect sacremoses\n",
            "Installing collected packages: keras-self-attention, pyyaml, keras-position-wise-feed-forward, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-embed-sim, tokenizers, sacremoses, keras-transformer, huggingface-hub, whoosh, transformers, syntok, sentencepiece, scikit-learn, langdetect, keras-bert, cchardet, ktrain\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\u001b[0m\n",
            "Successfully installed cchardet-2.1.7 huggingface-hub-0.8.1 keras-bert-0.89.0 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0 ktrain-0.31.2 langdetect-1.0.9 pyyaml-6.0 sacremoses-0.0.53 scikit-learn-0.24.2 sentencepiece-0.1.96 syntok-1.3.3 tokenizers-0.10.3 transformers-4.10.3 whoosh-2.7.4\n"
          ]
        }
      ],
      "source": [
        "!pip install ktrain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPfrjqSQWMdI"
      },
      "source": [
        "As usual, we import the packages after training them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qn0dEPbOCifq"
      },
      "outputs": [],
      "source": [
        "import ktrain\n",
        "from ktrain import text\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlagrY6L1g2K",
        "outputId": "40e51631-d0d9-48e0-adc9-63850a6b44bb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo1nd0ZmWbNH"
      },
      "source": [
        "## Importing and preparing the data\n",
        "\n",
        "We upload the dataset for training and validation. This time, a csv file with texts and their labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku9t8wEKECwB"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('Master_data_3.csv', header=0)\n",
        "\n",
        "#change names for columns of variables of interest, always keep text\n",
        "train_data = train_data[[\"text\", \"diversity\", \"moral\", \"market\", \"innovation\",\n",
        "                         \"ethnic_cultural\", \"gender\", \"sexual_ori\",\n",
        "                         \"age\", \"ability\", \"religion\"]]\n",
        "\n",
        "#train_data=train_data.dropna().reset_index(drop=True)"
      ],
      "metadata": {
        "id": "IF7ED5oAVLuG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = train_data.groupby('innovation').text.unique()\n",
        "# Sort the over-represented class to the head.\n",
        "labels = labels[labels.apply(len).sort_values(ascending=False).index]\n",
        "excess = len(labels.iloc[0]) - len(labels.iloc[1])\n",
        "remove = np.random.choice(labels.iloc[0], excess, replace=False)\n",
        "train_data = train_data[~train_data.text.isin(remove)]"
      ],
      "metadata": {
        "id": "A5CbKr2zUT_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data)"
      ],
      "metadata": {
        "id": "QuKmX6-UZGI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55fa7696-51d1-43cb-f775-5445826d5088"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   text diversity moral  \\\n",
            "0     The growing maturity of #fintech has led to an...        No   NaN   \n",
            "1     More investors are actively looking to invest ...       Yes    No   \n",
            "2     Today 50+ global CEOs signed a pledge to #Embr...       Yes    No   \n",
            "3     Steel is a truly diverse product, in both its ...        No   NaN   \n",
            "4     Who is talking about #diversity at #WEF19? Fol...       Yes    No   \n",
            "...                                                 ...       ...   ...   \n",
            "1181  Walking into your place of work with your head...        No    No   \n",
            "1182  We're proud to celebrate our 747 diverse new p...        No   Yes   \n",
            "1183  Congratulations to UvA students Priscilla Mari...        No    No   \n",
            "1184  We're very proud to announce our sponsorship &...        No   Yes   \n",
            "1185  We at Tata Steel understand the importance of ...        No    No   \n",
            "\n",
            "     market innovation ethnic_cultural gender sexual_ori  age ability religion  \n",
            "0       NaN        NaN             NaN    NaN        NaN  NaN     NaN      NaN  \n",
            "1        No         No              No     No         No   No      No       No  \n",
            "2        No         No              No     No         No   No      No       No  \n",
            "3       NaN        NaN             NaN    NaN        NaN  NaN     NaN      NaN  \n",
            "4        No         No              No     No         No   No      No       No  \n",
            "...     ...        ...             ...    ...        ...  ...     ...      ...  \n",
            "1181     No         No              No    Yes         No   No      No       No  \n",
            "1182     No         No              No     No         No   No      No       No  \n",
            "1183     No         No              No     No         No   No      No       No  \n",
            "1184     No         No              No     No         No   No      No       No  \n",
            "1185     No         No              No     No         No   No      No       No  \n",
            "\n",
            "[1186 rows x 11 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "702HzWXNWk72"
      },
      "source": [
        "*ktrain* is handy because it does a lot of things with a few lines of code. This part reads the data, transforms it into numbers using the BERT embeddings and the maximim sequence lenght, and seperates the training and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "3Iparw4eCifv",
        "outputId": "0ff3dbb7-5795-4379-82dd-2bcc50c5cff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No', 'Yes']\n",
            "       No  Yes\n",
            "1080  1.0  0.0\n",
            "737   1.0  0.0\n",
            "677   1.0  0.0\n",
            "816   0.0  1.0\n",
            "1075  0.0  1.0\n",
            "['No', 'Yes']\n",
            "      No  Yes\n",
            "72   1.0  0.0\n",
            "408  1.0  0.0\n",
            "198  0.0  1.0\n",
            "499  1.0  0.0\n",
            "74   1.0  0.0\n",
            "downloading pretrained BERT model (uncased_L-12_H-768_A-12.zip)...\n",
            "[██████████████████████████████████████████████████]\n",
            "extracting pretrained BERT model...\n",
            "done.\n",
            "\n",
            "cleanup downloaded zip...\n",
            "done.\n",
            "\n",
            "preprocessing train...\n",
            "language: en\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "done."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is Multi-Label? False\n",
            "preprocessing test...\n",
            "language: en\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "done."
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#change label_columns for variables of interest\n",
        "\n",
        "from ktrain.text.data import texts_from_csv\n",
        "(x_train, y_train), (x_test, y_test), preproc = text.texts_from_df(train_data,\n",
        "                      label_columns = [\"diversity\"],\n",
        "                      text_column = \"text\",\n",
        "                      preprocess_mode='bert',\n",
        "                      ngram_range=1,\n",
        "                      val_pct=0.1,\n",
        "                      maxlen=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZl9yA55XCW2"
      },
      "source": [
        "## Defining and training the model\n",
        "\n",
        "Here we create a BERT based text classifier. The main parameter to be changed here is the batch size (how many texts will the algorithm read at once)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjgetPrQCifx",
        "outputId": "13b56a30-b9c2-4897-c787-2442c8910c28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is Multi-Label? False\n",
            "maxlen is 128\n",
            "done.\n"
          ]
        }
      ],
      "source": [
        "learner = ktrain.get_learner(text.text_classifier('bert', (x_train, y_train), preproc=preproc, metrics = ['accuracy']),\n",
        "                             train_data=(x_train, y_train),\n",
        "                             val_data=(x_test, y_test),\n",
        "                             batch_size=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wi5Jb0mXQxt"
      },
      "source": [
        "This step trains the model. You already know what epochs mean, but what about the other parameters? This documentation may be handy: https://amaiya.github.io/ktrain/core.html#ktrain.core.Learner.autofit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeKUwrnYCifz",
        "outputId": "a7937c69-a635-400f-910f-ee8b3b38a0c1",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "begin training using triangular learning rate policy with max lr of 2e-05...\n",
            "Epoch 1/40\n",
            "89/89 [==============================] - 58s 390ms/step - loss: 0.5201 - accuracy: 0.7329 - val_loss: 0.3750 - val_accuracy: 0.8655\n",
            "Epoch 2/40\n",
            "89/89 [==============================] - 31s 347ms/step - loss: 0.2783 - accuracy: 0.8913 - val_loss: 0.3263 - val_accuracy: 0.8571\n",
            "Epoch 3/40\n",
            "89/89 [==============================] - 31s 352ms/step - loss: 0.2129 - accuracy: 0.9185 - val_loss: 0.3438 - val_accuracy: 0.8571\n",
            "Epoch 4/40\n",
            "89/89 [==============================] - 32s 358ms/step - loss: 0.1751 - accuracy: 0.9353 - val_loss: 0.4120 - val_accuracy: 0.8487\n",
            "Epoch 5/40\n",
            "89/89 [==============================] - 32s 356ms/step - loss: 0.1577 - accuracy: 0.9419 - val_loss: 0.4065 - val_accuracy: 0.8487\n",
            "Epoch 6/40\n",
            "89/89 [==============================] - 32s 356ms/step - loss: 0.1299 - accuracy: 0.9475 - val_loss: 0.4911 - val_accuracy: 0.8403\n",
            "Epoch 7/40\n",
            "89/89 [==============================] - ETA: 0s - loss: 0.1138 - accuracy: 0.9531\n",
            "Epoch 00007: Reducing Max LR on Plateau: new max lr will be 1e-05 (if not early_stopping).\n",
            "89/89 [==============================] - 32s 357ms/step - loss: 0.1138 - accuracy: 0.9531 - val_loss: 0.4986 - val_accuracy: 0.8403\n",
            "Epoch 8/40\n",
            "89/89 [==============================] - 32s 356ms/step - loss: 0.0904 - accuracy: 0.9616 - val_loss: 0.5074 - val_accuracy: 0.8319\n",
            "Epoch 9/40\n",
            "89/89 [==============================] - 32s 356ms/step - loss: 0.0803 - accuracy: 0.9663 - val_loss: 0.5353 - val_accuracy: 0.8235\n",
            "Epoch 10/40\n",
            "89/89 [==============================] - 32s 356ms/step - loss: 0.0754 - accuracy: 0.9672 - val_loss: 0.5881 - val_accuracy: 0.8151\n",
            "Epoch 11/40\n",
            "89/89 [==============================] - 32s 356ms/step - loss: 0.0703 - accuracy: 0.9672 - val_loss: 0.6107 - val_accuracy: 0.8487\n",
            "Epoch 12/40\n",
            "89/89 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9616\n",
            "Epoch 00012: Reducing Max LR on Plateau: new max lr will be 5e-06 (if not early_stopping).\n",
            "Restoring model weights from the end of the best epoch: 2.\n",
            "89/89 [==============================] - 32s 358ms/step - loss: 0.0697 - accuracy: 0.9616 - val_loss: 0.6186 - val_accuracy: 0.8235\n",
            "Epoch 12: early stopping\n",
            "Weights from best epoch have been loaded into model.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5d671b3510>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "learner.autofit(lr = 2e-5, early_stopping=10, reduce_on_plateau=5, epochs=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0byY7CbbYS2M"
      },
      "source": [
        "This gives you some additional information on where the classifier performed well (or not)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USM1eBJ3Cif0",
        "outputId": "108fccef-ee69-4cea-d6f4-fb8e54de94ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.88      0.89        82\n",
            "           1       0.75      0.81      0.78        37\n",
            "\n",
            "    accuracy                           0.86       119\n",
            "   macro avg       0.83      0.84      0.84       119\n",
            "weighted avg       0.86      0.86      0.86       119\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[72, 10],\n",
              "       [ 7, 30]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "learner.validate(val_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jWufDrfYbGz"
      },
      "source": [
        "## Making predictions\n",
        "\n",
        "The final blocks of code allow you to input text and see what the predicted label is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zkn-OdhQjqS5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e28d55c-4de5-4a93-fecd-444cf8c13a3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) Input-Token, Input-Segment with unsupported characters which will be renamed to input_token, input_segment in the SavedModel.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5df035e110> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5df02c06d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5df0170410> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5df0276ed0> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5df04a8b90> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5df01e0090> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5df0175210> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5df021f4d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5df0175890> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5df01e0cd0> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5df0073c50> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5df0096a50> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5df0294e90> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5d7c38de50> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5d7c391ed0> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5df03f7c10> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5d7c381390> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5df02ee2d0> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5d7c2b53d0> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5d7c378090> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5d7c23e290> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5d7c1fc550> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5d7c1fc6d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5d7c199090> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5df0291b50> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5d7c123f90> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5df03755d0> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5d7c068b10> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5df0122390> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5d7c07ee10> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5d673c9410> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5d673d2a10> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5d673e1150> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5d67318c10> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5d67314990> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5dfbb42510> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5d672965d0> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ],
      "source": [
        "learner.model.save('/content/drive/MyDrive/diversity_classifier/diversity_classifier')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "szwJK5R0jxMP"
      },
      "outputs": [],
      "source": [
        "learner.model.save_weights('/content/drive/MyDrive/diversity_classifier/diversity_classifier')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/diversity_classifier.zip /content/diversity_classifier"
      ],
      "metadata": {
        "id": "pytdNcs2xMn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoepWvyJBTD8"
      },
      "source": [
        "Loading model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cuwp_nBbBiV3"
      },
      "outputs": [],
      "source": [
        "learner.model.load('diversity_classifier')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyYPiBLGCif3"
      },
      "outputs": [],
      "source": [
        "predictor = ktrain.get_predictor(learner.model, preproc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJgeJM4gkQkL"
      },
      "outputs": [],
      "source": [
        "db = pd.read.csv('insertname', header=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUUpZoo4j4er"
      },
      "outputs": [],
      "source": [
        "predictions = predictor.predict(db.text.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnGRO8NmkBwH"
      },
      "outputs": [],
      "source": [
        "predictions_list = DataFrame(predictions, columns=['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7QctDWvkIZV"
      },
      "outputs": [],
      "source": [
        "predictions_list.to_csv('insertnameforsaving') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8z-iyv9Cif4"
      },
      "outputs": [],
      "source": [
        "prediction = predictor.predict(\"P\")\n",
        "print(prediction) "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DIV PSM",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}